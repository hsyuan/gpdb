#!/usr/bin/env python

import subprocess
import os
import platform
import sys
from optparse import OptionParser
from pygresql import pgdb
from datetime import datetime

gpsd_version = '%prog 1.0'
PATH_PREFIX = '/tmp/'
sysnslist = "('pg_toast', 'pg_bitmapindex', 'pg_temp_1', 'pg_catalog', 'information_schema', 'gp_toolkit')"
pgoptions = '-c gp_session_role=utility'

class MRQuery(object):
    def __init__(self):
        self.tables = {}
        self.funcs = {}
        self.tableoids = []


def generate_timestamp():
    timestamp = datetime.now()
    return timestamp.strftime("%Y%m%d%H%M%S")

def ResultIter(cursor, arraysize=1000):
    'An iterator that uses fetchmany to keep memory usage down'
    while True:
        results = cursor.fetchmany(arraysize)
        if not results:
            break
        for result in results:
            yield result


def getVersion(envOpts):
    cmd = subprocess.Popen('psql --pset footer -Atqc "select version()" template1', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=envOpts)
    if cmd.wait() is not 0:
        sys.stderr.write('\nError while trying to find HAWQ/GPDB version.\n\n' + cmd.communicate()[1] + '\n\n')
        sys.exit(1)
    return cmd.communicate()[0]


def dumpSchema(connectionInfo, envOpts):
    dmp_cmd = 'pg_dump -h %s -p %s -U %s -s -x --gp-syntax -O %s' % connectionInfo
    p = subprocess.Popen(dmp_cmd, shell=True, stderr=subprocess.PIPE, env=envOpts)
    if p.wait() is not 0:
        sys.stderr.write('\nError while dumping schema.\n\n' + p.communicate()[1] + '\n\n')
        sys.exit(1)


def parseCmdLine():
    p = OptionParser(usage='Usage: %prog <database> [options]', version=gpsd_version, conflict_handler="resolve")
    p.add_option('-?', '--help', action='help', help='Show this help message and exit')
    p.add_option('-h', '--host', action='store',
                 dest='host', help='Specify a remote host')
    p.add_option('-p', '--port', action='store',
                 dest='port', help='Specify a port other than 5432')
    p.add_option('-U', '--user', action='store', dest='user',
                 help='Connect as someone other than current user')
    p.add_option('-q', action='store', dest='query_file',
                 help='file name that contains the query')
    p.add_option('-f', action='store', dest='output_file',
                 help='minirepro output file name')
    p.add_option('-v', action='store_true', default='False', dest='dump_views',
                 help='dump all views in database')
    return p

def query_to_dxl(connectionInfo, query_file):
    (host, port, user, db) = connectionInfo
    print "Translating query file to xml ...: %s" % query_file

    file_in = os.path.abspath(query_file)
    basename = os.path.basename(query_file).split('.')[0]
    file_out = PATH_PREFIX + basename + '.xml'

    orca_udf_cmd = "psql %s -p %s -c \"select gpoptutils.DumpQueryFromFileToDXLFile('%s','%s')\"" % \
                   (db, port, file_in, file_out)

    print orca_udf_cmd

    envOpts = os.environ
    p = subprocess.Popen(orca_udf_cmd, shell=True, stderr=subprocess.PIPE ,env=envOpts)
    if p.wait() is not 0:
        sys.stderr.write('\nError while running ORCA UDFs.\n\n' + p.communicate()[1] + '\n\n')
        sys.exit(1)

    return file_out


def parse_xml(cursor, dxl_file):
    print "Parsing xml file ... : %s" % dxl_file
    result = MRQuery()
    with open(dxl_file, 'r') as f_dxl:
        for line in f_dxl:
            if line.strip().startswith('<dxl:TableDescriptor'):
                oid = line.split()[1].split('.')[1]
                result.tableoids.append(oid)

    oid_str = ','.join(result.tableoids)
    cat_query = "SELECT nspname, relname FROM pg_class, pg_namespace WHERE pg_class.relnamespace = pg_namespace.oid " \
                "AND pg_class.oid IN (%s)" % oid_str

    cursor.execute(cat_query)

    for vals in ResultIter(cursor):
        schm, table = vals[0], vals[1]
        if schm not in result.tables:
            result.tables[schm] = [table]
        else:
            result.tables[schm].append(table)

    return result

def dumpViews(cursor, connectionInfo, envOpts):
    query = "SELECT pgn.nspname, pgc.relname FROM pg_class pgc, pg_namespace pgn " \
        "WHERE pgn.oid = pgc.relnamespace and pgc.relkind = 'v' " \
        "and pgc.oid > 16758 and (pgn.oid > 16758 or pgn.oid = 2200)"
    cursor.execute(query)

    view_dict = {}
    for vals in ResultIter(cursor):
        schm, view = vals[0], vals[1]
        if schm not in view_dict:
            view_dict[schm] = [view]
        else:
            view_dict[schm].append(view)

    pg_dump_object(view_dict, 'view', connectionInfo, envOpts)

    return view_dict


def pg_dump_object(obj_dict, type_str, connectionInfo, envOpts):
    for schema, table_list in obj_dict.iteritems():
        table_str = "'" + schema + '.' + '|'.join(table_list) + "'"
        out_file = PATH_PREFIX + type_str + '_' + schema + '.dp.sql'
        dmp_cmd = 'pg_dump -h %s -p %s -U %s -sxO %s' % connectionInfo
        dmp_cmd += ' -t ' + table_str + ' -f ' + out_file
        print dmp_cmd
        p = subprocess.Popen(dmp_cmd, shell=True, stderr=subprocess.PIPE, env=envOpts)
        if p.wait() is not 0:
            sys.stderr.write('\nError while dumping schema.\n\n' + p.communicate()[1] + '\n\n')
            sys.exit(1)

def print_obj_DDL(filename, type_str, f_out):
    if filename.endswith('.dp.sql') and filename.startswith(type_str):
        f_path = os.path.join(PATH_PREFIX, filename)
        with open(f_path, 'r') as f_opened:
            line_no = 1
            for line in f_opened:
                if line_no == 12 or line_no > 16:
                    f_out.writelines(line)
                line_no += 1

def dumpTupleCount(cur, oid_str, f_out):
    stmt = "SELECT pgc.relname, pgn.nspname, pgc.relpages, pgc.reltuples FROM pg_class pgc, pg_namespace pgn " \
            "WHERE pgc.relnamespace = pgn.oid and pgc.oid in (%s) and pgn.nspname NOT IN %s" % (oid_str, sysnslist)

    setStmt = '-- Table: {1}\n' \
        'UPDATE pg_class\nSET\n' \
        '{0}\n' \
        'WHERE relname = \'{1}\' AND relnamespace = ' \
        '(SELECT oid FROM pg_namespace WHERE nspname = \'{2}\');\n\n'

    cur.execute(stmt)
    columns = map(lambda x: x[0], cur.description)
    types = ['int', 'real']
    for vals in ResultIter(cur):
        f_out.writelines(setStmt.format(',\n'.join(map(lambda t: '\t%s = %s::%s' %
                                            t, zip(columns[2:], vals[2:], types))), vals[0], vals[1]))

def dumpStats(cur, oid_str, f_out):
    query = 'SELECT pgc.relname, pgn.nspname, pga.attname, pgt.typname, pgs.* ' \
        'FROM pg_class pgc, pg_statistic pgs, pg_namespace pgn, pg_attribute pga, pg_type pgt ' \
        'WHERE pgc.relnamespace = pgn.oid and pgc.oid in (%s) and pgn.nspname NOT IN %s' \
        ' and pgc.oid = pgs.starelid ' \
        'and pga.attrelid = pgc.oid ' \
        'and pga.attnum = pgs.staattnum ' \
        'and pga.atttypid = pgt.oid ' \
        'ORDER BY pgc.relname, pgs.staattnum' % (oid_str, sysnslist)
    pstring = '--\n' \
        '-- Table: {0}, Attribute: {1}\n' \
        '--\n' \
        'INSERT INTO pg_statistic VALUES (\n' \
        '{2});\n\n'
    types = ['smallint',  # staattnum
             'real',
             'integer',
             'real',
             'smallint',
             'smallint',
             'smallint',
             'smallint',
             'oid',
             'oid',
             'oid',
             'oid',
             'real[]',
             'real[]',
             'real[]',
             'real[]'
             ]

    cur.execute(query)

    for vals in ResultIter(cur):
        rowVals = ["\t'%s.%s'::regclass" % tuple(vals[1::-1])]

        if vals[3][0] == '_':
            rowTypes = types + [vals[3]] * 4
        else:
            rowTypes = types + [vals[3] + '[]'] * 4
        for val, typ in zip(vals[5:], rowTypes):
            if val is None:
                val = 'NULL'
            elif isinstance(val, (str, unicode)) and val[0] == '{':
                val = val.replace("'", "''").replace('\\', '\\\\')
                val = "E'" + val + "'"
            rowVals.append('\t{0}::{1}'.format(val, typ))
        f_out.writelines(pstring.format(vals[0], vals[2], ',\n'.join(rowVals)))

def main():
    parser = parseCmdLine()
    options, args = parser.parse_args()
    if len(args) != 1:
        parser.error("No database specified")
        exit(1)

    # OK - now let's setup all the arguments & options
    envOpts = os.environ
    db = args[0]
    host = options.host or platform.node()
    user = options.user or os.getlogin()
    port = options.port or envOpts['PGPORT'] or '5432'
    query_file = options.query_file
    output_file = options.output_file
    dump_views = options.dump_views

    if query_file is None:
        raise Exception("No query file specified. See minirepro -? for usage.")
    if output_file is None:
        raise Exception("No output file specified. See minirepro -? for usage.")

    # check if db conn is ok before we do anything real
    version = getVersion(envOpts)

    timestamp = generate_timestamp()
    global PATH_PREFIX
    PATH_PREFIX = PATH_PREFIX + timestamp + '/'

    # Create tmp dir if not already there
    try:
        os.stat(PATH_PREFIX)
    except:
        os.mkdir(PATH_PREFIX)

    # setup the connection info tuple with options
    connectionInfo = (host, port, user, db)
    connectionString = ':'.join([host, port, db, user, '', pgoptions, ''])
    print "Connecting to database: host=%s, user=%s, port=%s, db=%s ..." % connectionInfo
    conn = pgdb.connect(connectionString)
    cursor = conn.cursor()

    """
    invoke Orca UDFs, put xml file in a temp folder
    input: query file name
    output: file name of the xml file
    """
    xml_file = query_to_dxl(connectionInfo, query_file)

    """
    parse xml file, collect all things that need to be dumped
    input: xml file name
    output: MRQuery class (self.tables, self.views, self.funcs, etc)
    """
    mr_query = parse_xml(cursor, xml_file)

    # dump tables
    print "Invoking pg_dump to dump DDL ..."
    pg_dump_object(mr_query.tables, 'table', connectionInfo, envOpts)

    # dump functions


    # dump all the views in db
    view_dict = {}
    if dump_views:
        view_dict = dumpViews(cursor, connectionInfo, envOpts)

    ### start writing out to stdout ###
    f_out = open(output_file, 'w')
    ts = datetime.today()
    f_out.writelines(['-- MiniRepro 1.0',
                           '\n-- Copyright (C) 2007 - 2015 Pivotal'
                           '\n-- Database: ' + db,
                           '\n-- Date:     ' + ts.date().isoformat(),
                           '\n-- Time:     ' + ts.time().isoformat(),
                           '\n-- CmdLine:  ' + ' '.join(sys.argv),
                           '\n-- Version:  ' + version + '\n\n'])

    # Now be sure that when we load the rest we are doing it in the right
    # database
    f_out.writelines('\\connect ' + db + '\n\n')

    # first create schemas
    print "Creating schemas ..."
    table_schemas = ["CREATE SCHEMA %s;\n" % schema for schema in mr_query.tables if schema != 'public']
    f_out.writelines(table_schemas)
    view_schemas = ["CREATE SCHEMA %s;\n" % schema for schema in view_dict if schema != 'public']
    f_out.writelines(view_schemas)

    # concatenate table DDLs
    print "Writing table DDLs ..."
    for f in os.listdir(PATH_PREFIX):
        print_obj_DDL(f, 'table', f_out)

    # concatenate view DDLs
    print "Writing view DDLs ..."
    for f in os.listdir(PATH_PREFIX):
        print_obj_DDL(f, 'view', f_out)

    # Now we have to explicitly allow editing of these pg_class &
    # pg_statistic tables
    f_out.writelines(['\n-- ',
                           '\n-- Allow system table modifications',
                           '\n-- ',
                           '\nset allow_system_table_mods="DML";\n\n'])


    # dump table stats
    print "Writing table statistics ..."
    oid_str = ','.join(mr_query.tableoids)
    dumpTupleCount(cursor, oid_str, f_out)

    # dump column stats
    print "Writing column statistics ..."
    dumpStats(cursor, oid_str, f_out)

    cursor.close()
    conn.close()

    # attach query text
    print "Attaching raw query text ..."
    f_out.writelines(['\n-- ',
                       '\n-- Query text',
                       '\n-- '])

    f_out.writelines('\n/*\n\n')
    with open(query_file, 'r') as query_f:
        for line in query_f:
            f_out.writelines(line)
    f_out.writelines('\n*/\n')

    f_out.writelines('\n-- MiniRepro completed.\n')
    f_out.close()

    print "--- MiniRepro completed! ---"

if __name__ == "__main__":
        main()

